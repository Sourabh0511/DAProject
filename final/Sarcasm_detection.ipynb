{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import random\n",
    "import math\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import neighbors \n",
    "from sklearn.metrics import f1_score\n",
    "from keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Convolution1D, Flatten\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern = \"((http|ftp|https):\\/\\/)?([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w.,@?^=%&:\\/~+#-])?\"\n",
    "    return re.sub(pattern, \"\", text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    removed_stopwords = \" \".join([w for w in tokens if not w in stop_words])\n",
    "    return removed_stopwords\n",
    "\n",
    "def is_too_short(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return len(tokens) <= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def divide_text(text, n):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    spilting_length = len(tokens) / n\n",
    "    out = []\n",
    "    x = 0\n",
    "    for i in range(n):\n",
    "        str_list = tokens[x:int(x+spilting_length)]\n",
    "        string = \" \".join(str_list)\n",
    "        out.append(string)\n",
    "        x = int(x+spilting_length)\n",
    "    return out\n",
    "\n",
    "def get_sentiment(arr):\n",
    "    n = len(arr)\n",
    "    polar = []\n",
    "    for i in range(n):\n",
    "        polar.append(TextBlob(arr[i]).sentiment.polarity)\n",
    "    return polar\n",
    "\n",
    "def find_sentiment(arr):\n",
    "    n = len(arr)\n",
    "    #Returns new array of same dimension without initialisation\n",
    "    out = np.empty((len(arr), 6))\n",
    "    for i in range(len(arr)):\n",
    "        uni_polarity = TextBlob(arr[i]).sentiment.polarity\n",
    "        bigrams_list = divide_text(arr[i], 2)\n",
    "        bi_polarity = get_sentiment(bigrams_list)\n",
    "        trigram_list = divide_text(arr[i], 3)\n",
    "        tri_polarity = get_sentiment(trigram_list)\n",
    "        out[i] = [uni_polarity, bi_polarity[0], bi_polarity[1], tri_polarity[0], tri_polarity[1], tri_polarity[2]]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_features(text):\n",
    "    def count_apost(text):\n",
    "            return text.count('!')\n",
    "    def count_qn(text):\n",
    "            return text.count('?')\n",
    "    def count_capitals(text):\n",
    "            count=0\n",
    "            tokens=nltk.word_tokenize(text)\n",
    "            for each_word in tokens:\n",
    "                if each_word[0].isupper():\n",
    "                    count+=1\n",
    "            return count\n",
    "    def data_len(text):\n",
    "            return len(text)\n",
    "    def count_quotes(text):\n",
    "            return text.count('\\\"')\n",
    "    def count_emoji(text):\n",
    "        emoji_list = [\":p\", \":)\", \";)\", \"emoticonX\"]\n",
    "        emoji_count = [text.count(e) for e in emoji_list]\n",
    "        return sum(emoji_count)\n",
    "    \n",
    "    ap = count_apost(text)\n",
    "    qn = count_qn(text)\n",
    "    cap = count_capitals(text)\n",
    "    l = data_len(text)\n",
    "    quotes = count_quotes(text)\n",
    "    #emo = count_emoji(text)\n",
    "    return np.array([ ap, qn , cap , l , quotes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_tag_finder(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    counts = Counter(tag for word,tag in tags)\n",
    "    total = sum(counts.values())\n",
    "    return dict((word, float(count)/total) for word,count in counts.items())\n",
    "\n",
    "def get_pos_features(arr):\n",
    "    out = np.array([])\n",
    "    for i in range(len(arr)):\n",
    "        pos_tags = pos_tag_finder(arr[i])\n",
    "        out = np.append(out, pos_tags)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "dataset = load_files('databin/', encoding=\"utf8\", decode_error=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([])\n",
    "y = np.array([])\n",
    "for i in range(len(dataset.data)):\n",
    "    if not is_too_short(dataset.data[i]):\n",
    "        noisless_text = remove_url(str(dataset.data[i]))\n",
    "        noisless_text = remove_stopwords(noisless_text)\n",
    "        #noisless_text = TextBlob(noisless_text).correct()\n",
    "        X = np.append(X, noisless_text)\n",
    "        if dataset.target[i] == 0:\n",
    "            y = np.append(y, 'notsarc')\n",
    "        else:\n",
    "            y = np.append(y, 'sarc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "#Getting features of length of text ,count of capitals, apostrophe, question marks, quotes, abbreviation\n",
    "feat_count_train=np.array([])\n",
    "f=np.array([])\n",
    "feat_count_test = np.array([])\n",
    "for x in X_train:\n",
    "    f = count_features(x)\n",
    "    feat_count_train = np.append(feat_count_train ,f )\n",
    "feat_count_train = feat_count_train.reshape(len(X_train) , 5)\n",
    "for x in X_test:\n",
    "    f = count_features(x)\n",
    "    feat_count_test = np.append(feat_count_test ,f )\n",
    "feat_count_test = feat_count_test.reshape(len(X_test) , 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Returns unigram, bi gram and trigram polarity\n",
    "sentiment_train = csr_matrix(find_sentiment(X_train))\n",
    "sentiment_test = csr_matrix(find_sentiment(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stem reduces size of dictionary by converting words to their root form\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the tfidf matrix(count of words per sentence)\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english', ngram_range=(1, 5), max_features=1939)\n",
    "tfs_train = tfidf.fit_transform(X_train)\n",
    "tfs_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scikit-learn estimators expect numerical features, we convert the categorical and boolean features using one-hot encoding\n",
    "#In order to perform one-hot encoding, we need to give the entire data as an input and cannot perform one-hot encoding per observation. \n",
    "vec = DictVectorizer()\n",
    "\n",
    "pos_train = vec.fit_transform(get_pos_features(X_train))\n",
    "pos_test = vec.transform(get_pos_features(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#To split the text into topics and use these topics as features, (some topics are more likely to be sarcastic)\n",
    "lda = LatentDirichletAllocation(n_topics=10, learning_method='online')\n",
    "#for some versions of python n_topics as parameter name should be changed to n_components if it throws any error\n",
    "\n",
    "topic_train = lda.fit_transform(tfs_train)\n",
    "topic_test = lda.transform(tfs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3269, 2004)\n"
     ]
    }
   ],
   "source": [
    "#COMBINE ALL THE ABOVE FEATURES\n",
    "final_train = hstack([sentiment_train, tfs_train, pos_train, topic_train, feat_count_train])\n",
    "final_test = hstack([sentiment_test, tfs_test, pos_test, topic_test, feat_count_test])\n",
    "#print(final_train)\n",
    "print(final_train.shape)\n",
    "accuracies = []\n",
    "f_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.681169757489\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    notsarc       0.75      0.56      0.64       711\n",
      "       sarc       0.64      0.80      0.71       691\n",
      "\n",
      "avg / total       0.69      0.68      0.68      1402\n",
      "\n",
      "0.677120798782\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "#1. Bernoulli\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "Bnb_clf = BernoulliNB()\n",
    "Bnb_clf = Bnb_clf.fit(final_train.toarray(), y_train)\n",
    "predict = Bnb_clf.predict(final_test.toarray())\n",
    "print(accuracy_score(y_test, predict))\n",
    "print(classification_report(y_test, predict))\n",
    "accuracies.append(accuracy_score(y_test, predict))\n",
    "f_scores.append(f1_score(y_test, predict,average = \"macro\"))\n",
    "print(f1_score(y_test, predict,average = \"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.644793152639\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    notsarc       0.65      0.65      0.65       711\n",
      "       sarc       0.64      0.64      0.64       691\n",
      "\n",
      "avg / total       0.64      0.64      0.64      1402\n",
      "\n",
      "0.64470566683\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "#2. Gaussian\n",
    "gnb_clf = GaussianNB()\n",
    "gnb_clf = gnb_clf.fit(final_train.toarray(), y_train)\n",
    "predict = gnb_clf.predict(final_test.toarray())\n",
    "print(accuracy_score(y_test, predict))\n",
    "print(classification_report(y_test, predict))\n",
    "accuracies.append(accuracy_score(y_test, predict))\n",
    "f_scores.append(f1_score(y_test, predict,average = \"macro\"))\n",
    "print(f1_score(y_test, predict,average = \"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.696861626248\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    notsarc       0.72      0.67      0.69       711\n",
      "       sarc       0.68      0.73      0.70       691\n",
      "\n",
      "avg / total       0.70      0.70      0.70      1402\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.69674915707106055"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "logistic_clf = LogisticRegression()\n",
    "logistic_clf = logistic_clf.fit(final_train, y_train)\n",
    "predict = logistic_clf.predict(final_test)\n",
    "print(accuracy_score(y_test, predict))\n",
    "print(classification_report(y_test, predict))\n",
    "accuracies.append(accuracy_score(y_test, predict))\n",
    "f_scores.append(f1_score(y_test, predict,average = \"macro\"))\n",
    "f1_score(y_test, predict,average = \"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.545649072753\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    notsarc       0.54      0.68      0.60       711\n",
      "       sarc       0.55      0.40      0.47       691\n",
      "\n",
      "avg / total       0.55      0.55      0.54      1402\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.53552376667854196"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM WITH RBF KERNEL\n",
    "svm_clf = SVC(C=4,gamma=1.3)\n",
    "svm_clf = svm_clf.fit(final_train, y_train)\n",
    "predict = svm_clf.predict(final_test)\n",
    "print(accuracy_score(y_test, predict))\n",
    "print(classification_report(y_test, predict))\n",
    "accuracies.append(accuracy_score(y_test, predict))\n",
    "f_scores.append(f1_score(y_test, predict,average = \"macro\"))\n",
    "f1_score(y_test, predict,average = \"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.597717546362\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    notsarc       0.56      0.91      0.70       711\n",
      "       sarc       0.75      0.27      0.40       691\n",
      "\n",
      "avg / total       0.66      0.60      0.55      1402\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.54965828321487142"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lINEAR SVM\n",
    "linear_svm_clf = LinearSVC(C=0.1)\n",
    "linear_svm_clf = linear_svm_clf.fit(final_train, y_train)\n",
    "predict = linear_svm_clf.predict(final_test)\n",
    "print(accuracy_score(y_test, predict))\n",
    "print(classification_report(y_test, predict))\n",
    "accuracies.append(accuracy_score(y_test, predict))\n",
    "f_scores.append(f1_score(y_test, predict,average = \"macro\"))\n",
    "f1_score(y_test, predict,average = \"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.594864479315\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    notsarc       0.60      0.61      0.60       711\n",
      "       sarc       0.59      0.58      0.58       691\n",
      "\n",
      "avg / total       0.59      0.59      0.59      1402\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.59462607307498083"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decision Tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(final_train, y_train)\n",
    "predict = clf.predict(final_test)\n",
    "print(accuracy_score(y_test, predict))\n",
    "print(classification_report(y_test, predict))\n",
    "accuracies.append(accuracy_score(y_test, predict))\n",
    "f_scores.append(f1_score(y_test, predict,average = \"macro\"))\n",
    "f1_score(y_test, predict,average = \"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#PLOT\n",
    "import graphviz \n",
    "with open(\"sarcasm_dt.txt\", \"w\") as f:\n",
    "    f = tree.export_graphviz(clf, out_file=f, max_depth =3)\n",
    "#Use the text file to plot the tree on http://webgraphviz.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.679029957204\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    notsarc       0.71      0.62      0.66       711\n",
      "       sarc       0.66      0.74      0.69       691\n",
      "\n",
      "avg / total       0.68      0.68      0.68      1402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=100)\n",
    "rf_clf = rf_clf.fit(final_train, y_train)\n",
    "predict = rf_clf.predict(final_test)\n",
    "print(accuracy_score(y_test, predict))\n",
    "print(classification_report(y_test, predict))\n",
    "accuracies.append(accuracy_score(y_test, predict))\n",
    "f_scores.append(f1_score(y_test, predict,average = \"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniform\n",
      "0.581312410842\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    notsarc       0.58      0.62      0.60       711\n",
      "       sarc       0.58      0.54      0.56       691\n",
      "\n",
      "avg / total       0.58      0.58      0.58      1402\n",
      "\n",
      "0.580410517294\n",
      "distance\n",
      "0.597004279601\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    notsarc       0.60      0.61      0.61       711\n",
      "       sarc       0.59      0.58      0.59       691\n",
      "\n",
      "avg / total       0.60      0.60      0.60      1402\n",
      "\n",
      "0.596780884849\n"
     ]
    }
   ],
   "source": [
    "#KNN (N=21 GAVE BEST ACCURACY)\n",
    "n_neighbours = 21\n",
    "for weights in ['uniform', 'distance']:\n",
    "    clf= neighbors.KNeighborsClassifier(n_neighbours, weights = weights)\n",
    "    clf.fit(final_train , y_train)\n",
    "    predict = clf.predict(final_test)\n",
    "    print(weights)\n",
    "    print(accuracy_score(y_test, predict))\n",
    "    print(classification_report(y_test, predict))\n",
    "    accuracies.append(accuracy_score(y_test, predict))\n",
    "    f_scores.append(f1_score(y_test, predict,average = \"macro\"))\n",
    "    print(f1_score(y_test, predict,average = \"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12867\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "#Recurrent neural net\n",
    "#Training\n",
    "xlist = list(X_train)\n",
    "#print(xlist)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(xlist)\n",
    "print(len(tokenizer.word_index))\n",
    "sequences = tokenizer.texts_to_sequences(xlist)\n",
    "#print(sequences)\n",
    "l = len(max(sequences,key = lambda  x : len(x)))\n",
    "print(l)\n",
    "padded_sequences = pad_sequences(sequences, maxlen = 1000) #padded_sequencies is the tokenized and padded data\n",
    "#padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index)+1, 128, input_length=1000)) #maxlen of tokenizerwordindex\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2)) #128 depends on no of words in a row \n",
    "model.add(Dense(2, activation='sigmoid')) #2 because of one hot enc\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_new = []\n",
    "y_test_new = []\n",
    "for x in y_train:\n",
    "    if x == 'sarc':\n",
    "        y_train_new.append(1)\n",
    "    else:\n",
    "        y_train_new.append(0)\n",
    "for x in y_test:\n",
    "    if x == 'sarc':\n",
    "        y_test_new.append(1)\n",
    "    else:\n",
    "        y_test_new.append(0)\n",
    "        \n",
    "                \n",
    "#print(y_train_new)\n",
    "y_train_new = to_categorical(y_train_new, num_classes = 2)\n",
    "y_test_new = to_categorical(y_test_new,  num_classes = 2) \n",
    "#print(y_train_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2615 samples, validate on 654 samples\n",
      "Epoch 1/3\n",
      "2615/2615 [==============================] - 212s - loss: 0.6735 - acc: 0.5996 - val_loss: 0.6397 - val_acc: 0.6514\n",
      "Epoch 2/3\n",
      "2615/2615 [==============================] - 217s - loss: 0.4881 - acc: 0.7868 - val_loss: 0.6259 - val_acc: 0.6797\n",
      "Epoch 3/3\n",
      "2615/2615 [==============================] - 193s - loss: 0.2402 - acc: 0.9059 - val_loss: 0.7913 - val_acc: 0.6399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb4eb9be080>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_sequences, y_train_new, validation_split=0.2, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12867\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "xlist_test = list(X_test)\n",
    "#print(xlist)\n",
    "\n",
    "\n",
    "#tokenizer = Tokenizer()\n",
    "#tokenizer.fit_on_texts(xlist_test)\n",
    "print(len(tokenizer.word_index))\n",
    "sequences = tokenizer.texts_to_sequences(xlist_test)\n",
    "#print(sequences)\n",
    "l_test = len(max(sequences,key = lambda  x : len(x)))\n",
    "print(l_test)\n",
    "padded_sequences_test = pad_sequences(sequences, maxlen = 1000) #padded_sequencies is the tokenized and padded data\n",
    "#padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.34%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(padded_sequences_test,y_test_new,verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "#y_pred = model.predict()\n",
    "accuracies.append(scores[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "objects = ('NB Bernoulli', 'NB Gaussian', 'Logistic regression',  'SVM RBF', 'SVM Linear','Decision trees', 'Random Forests', 'kNN', 'RNN')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = accuracies[0:9]\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects, rotation=90)\n",
    "\n",
    "plt.ylabel('Classification Model')\n",
    "plt.title('Comparison of accuracies')\n",
    "plt.savefig(\"accuracies.jpg\") \n",
    "plt.close()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "objects = ('NB Bernoulli', 'NB Gaussian', 'Logistic regression' 'Decision trees', 'Random Forests', 'SVM RBF', 'SVM Linear', 'kNN', 'RNN')\n",
    "print(len(objects))\n",
    "print(len(accuracies[0:8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "objects = ('NB Bernoulli', 'NB Gaussian', 'Logistic regression',  'SVM RBF', 'SVM Linear','Decision trees', 'Random Forests', 'kNN')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance_fscores = f_scores[0:8]\n",
    " \n",
    "plt.bar(y_pos, performance_fscores, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects, rotation=90)\n",
    "\n",
    "plt.ylabel('Classification Model')\n",
    "plt.title('Comparison of f1_scores')\n",
    "plt.savefig(\"f1_scores.jpg\") \n",
    "plt.close()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
